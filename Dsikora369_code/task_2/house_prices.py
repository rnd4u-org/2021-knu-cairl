# -*- coding: utf-8 -*-
"""House Prices.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tkV80qKX6G9H_FgndzNtVvfi4zI-1iMU

##Kaggle House Prices Denys Sikorskyi##

**Loading data**

Downloading Kaggle token.
"""

import pandas as pd
import numpy as np
from google.colab import files
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.neighbors import KNeighborsRegressor
from catboost import CatBoostRegressor
import skew
from sklearn.metrics import mean_squared_error
files.upload()  # here you will download kaggle.json

"""Setting permission before downloading dataset."""


"""**EDA**

First, let's look at our dataset.
"""

train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')
train.head()

"""Using correlation matrix."""

# Correlation Matrix

f, ax = plt.subplots(figsize=(30, 25))
mat = train.corr('pearson')
mask = np.triu(np.ones_like(mat, dtype=bool))
cmap = sns.diverging_palette(230, 20, as_cmap=True)
sns.heatmap(mat, mask=mask, cmap=cmap, vmax=1, center=0, annot=True,
            square=True, linewidths=.5, cbar_kws={"shrink": .5})
plt.show()

"""**Data Preprocessing**

Analyzing NaNs.
"""

target = train['SalePrice']
test_id = test['Id']
test = test.drop(['Id'], axis=1)
train_2 = train.drop(['SalePrice', 'Id'], axis=1)

# Concatenating train & test set

train_test = pd.concat([train_2, test], axis=0, sort=False)

# Looking at NaN % within the data

nan = pd.DataFrame(train_test.isna().sum(), columns=['NaN_sum'])
nan['feat'] = nan.index
nan['Perc(%)'] = (nan['NaN_sum'] / 1460) * 100
nan = nan[nan['NaN_sum'] > 0]
nan = nan.sort_values(by=['NaN_sum'])
nan['Usability'] = np.where(nan['Perc(%)'] > 20, 'Discard', 'Keep')
nan

"""After reading data_description we can that some of NaNs are not missing information, so we should transform them to categorical."""

# Converting non-numeric predictors stored as numbers into string

train_test['MSSubClass'] = train_test['MSSubClass'].apply(str)
train_test['YrSold'] = train_test['YrSold'].apply(str)
train_test['MoSold'] = train_test['MoSold'].apply(str)

# Filling Categorical NaN

train_test['Functional'] = train_test['Functional'].fillna('Typ')
train_test['Electrical'] = train_test['Electrical'].fillna("SBrkr")
train_test['KitchenQual'] = train_test['KitchenQual'].fillna("TA")
train_test['Exterior1st'] = train_test['Exterior1st'].fillna(
    train_test['Exterior1st'].mode()[0])
train_test['Exterior2nd'] = train_test['Exterior2nd'].fillna(
    train_test['Exterior2nd'].mode()[0])
train_test['SaleType'] = train_test['SaleType'].fillna(
    train_test['SaleType'].mode()[0])
train_test["PoolQC"] = train_test["PoolQC"].fillna("None")
train_test["Alley"] = train_test["Alley"].fillna("None")
train_test['FireplaceQu'] = train_test['FireplaceQu'].fillna("None")
train_test['Fence'] = train_test['Fence'].fillna("None")
train_test['MiscFeature'] = train_test['MiscFeature'].fillna("None")

for col in ('GarageArea', 'GarageCars'):
    train_test[col] = train_test[col].fillna(0)

for col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:
    train_test[col] = train_test[col].fillna('None')

for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1',
            'BsmtFinType2'):
    train_test[col] = train_test[col].fillna('None')

# Checking the features with NaN remained out

for col in train_test:
    if train_test[col].isna().sum() > 0:
        print(train_test[col][0])


# Imputing with KnnRegressor (we can also use different Imputers)

def impute_knn(df):
    ttn = train_test.select_dtypes(include=[np.number])
    ttc = train_test.select_dtypes(exclude=[np.number])

    cols_nan = ttn.columns[ttn.isna().any()].tolist()  # columns w/ nan
    cols_no_nan = ttn.columns.difference(cols_nan).values  # columns w/n nan

    for col in cols_nan:
        imp_test = ttn[ttn[col].isna()]
        imp_train = ttn.dropna()
        model = KNeighborsRegressor(n_neighbors=5)
        knr = model.fit(imp_train[cols_no_nan], imp_train[col])
        ttn.loc[ttn[col].isna(), col] = knr.predict(imp_test[cols_no_nan])

    return pd.concat([ttn, ttc], axis=1)


train_test = impute_knn(train_test)


objects = []
for i in train_test.columns:
    if train_test[i].dtype == object:
        objects.append(i)
train_test.update(train_test[objects].fillna('None'))

# Checking NaN presence

for col in train_test:
    if train_test[col].isna().sum() > 0:
        print(train_test[col][0])


train_test["SqFtPerRoom"] = (train_test["GrLivArea"] / (train_test[
    "TotRmsAbvGrd"] + train_test["FullBath"] + train_test[
        "HalfBath"] + train_test["KitchenAbvGr"]))

train_test['Total_Home_Quality'] = (train_test[
    'OverallQual'] + train_test['OverallCond'])

train_test['Total_Bathrooms'] = (train_test[
    'FullBath'] + (0.5 * train_test['HalfBath']) + train_test[
        'BsmtFullBath'] + (0.5 * train_test['BsmtHalfBath']))

train_test["HighQualSF"] = train_test["1stFlrSF"] + train_test["2ndFlrSF"]

# Converting non-numeric predictors stored as numbers into string

train_test['MSSubClass'] = train_test['MSSubClass'].apply(str)
train_test['YrSold'] = train_test['YrSold'].apply(str)
train_test['MoSold'] = train_test['MoSold'].apply(str)

# Creating dummy variables from categorical features

train_test_dummy = pd.get_dummies(train_test)

# Fetch all numeric features

# train_test['Id'] = train_test['Id'].apply(str)
numeric_features = train_test_dummy.dtypes[
    train_test_dummy.dtypes != object].index
skewed_features = train_test_dummy[numeric_features].apply(
    lambda x: skew(x)).sort_values(ascending=False)
high_skew = skewed_features[skewed_features > 0.5]
skew_index = high_skew.index

# Normalize skewed features using log_transformation

for i in skew_index:
    train_test_dummy[i] = np.log1p(train_test_dummy[i])

"""**Modeling**"""

train = train_test_dummy[0:1460]
test = train_test_dummy[1460:]
test['Id'] = test_id

target_log = np.log1p(target)

X_train, X_val, y_train, y_val = train_test_split(train, target_log,
                                                  test_size=0.1,
                                                  random_state=42)

# Cat Boost Regressor

cat = CatBoostRegressor()
cat_model = cat.fit(X_train, y_train, eval_set=(X_val, y_val), plot=True,
                    verbose=0)


def rmse(y, y_pred):
    return np.sqrt(mean_squared_error(y, y_pred))


cat_pred = cat_model.predict(X_val)
cat_score = rmse(y_val, cat_pred)
cat_score

"""Feature importance."""

feat_imp = cat_model.get_feature_importance(prettified=True)
feat_imp

"""**Hyperparameter Optimization**"""

# Preforming a Random Grid Search to find the best combination of parameters

grid = {'iterations': [1000, 3000, 6000],
        'learning_rate': [0.0005 + 0.0005 * i for i in range(0, 50)],
        'depth': [4, 6, 10],
        'l2_leaf_reg': [1, 3, 5, 9]}

final_model = CatBoostRegressor()
randomized_search_result = final_model.randomized_search(grid, X=X_train,
                                                         y=y_train,
                                                         verbose=False,
                                                         plot=True)

"""Final model."""

print("\n========================================================")
print(" Results from Grid Search ")
print("========================================================")
print("\n The best params across ALL searched params:\n",
      randomized_search_result['params'])

# Final Cat-Boost Regressor

params = {'iterations': randomized_search_result['params']['iterations'],
          'learning_rate': randomized_search_result['params']['learning_rate'],
          'depth': randomized_search_result['params']['depth'],
          'l2_leaf_reg': randomized_search_result['params']['l2_leaf_reg'],
          'eval_metric': 'RMSE',
          'early_stopping_rounds': 200,
          'verbose': 200,
          'random_seed': 17}

cat_f = CatBoostRegressor(**params)
cat_model_f = cat_f.fit(X_train, y_train, eval_set=(X_val, y_val),
                        plot=True, verbose=False)

catf_pred = cat_model_f.predict(X_val)
catf_score = rmse(y_val, catf_pred)
catf_score

"""**Saving model and submission**"""

# Commented out IPython magic to ensure Python compatibility.
# %%flake8
# # Test CSV Submission
# cat_model_f.save_model('House_Price_model')
# test_pred = cat_f.predict(test)
# submission = pd.DataFrame(test_id, columns=['Id'])
# test_pred = np.expm1(test_pred)
# submission['SalePrice'] = test_pred
# submission.head()
# submission.to_csv("result.csv", index=False, header=True)
# print('Done!')
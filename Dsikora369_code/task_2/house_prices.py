# -*- coding: utf-8 -*-
"""House Prices.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tkV80qKX6G9H_FgndzNtVvfi4zI-1iMU

##Kaggle House Prices Denys Sikorskyi##

**Loading data**

Downloading Kaggle token.
"""

import pandas as pd
import numpy as np
import re
from google.colab import files
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
import tensorflow as tf
import math
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.layers import Dense, Dropout
from sklearn.neighbors import KNeighborsRegressor
from scipy.stats import skew, norm
import xgboost as xgb
from catboost import Pool
from sklearn.svm import SVR
from catboost import CatBoostRegressor
from lightgbm import LGBMRegressor
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeRegressor
from mlxtend.regressor import StackingRegressor
from sklearn.linear_model import LinearRegression, BayesianRidge
from sklearn.model_selection import RepeatedKFold
from sklearn.model_selection import KFold, cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_log_error
files.upload()  # here you will download kaggle.json

"""Setting permission before downloading dataset."""

!pip install -q kaggle
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!ls ~/.kaggle
!chmod 600 /root/.kaggle/kaggle.json  # set permission

"""Downloading dataset."""

!kaggle competitions download -c house-prices-advanced-regression-techniques

"""**EDA**

First, let's look at our dataset.
"""

train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')
train.head()

"""Using correlation matrix."""

# Correlation Matrix

f, ax = plt.subplots(figsize=(30, 25))
mat = train.corr('pearson')
mask = np.triu(np.ones_like(mat, dtype=bool))
cmap = sns.diverging_palette(230, 20, as_cmap=True)
sns.heatmap(mat, mask=mask, cmap=cmap, vmax=1, center=0, annot=True,
            square=True, linewidths=.5, cbar_kws={"shrink": .5})
plt.show()

"""**Data Preprocessing**

Analyzing NaNs.
"""

target = train['SalePrice']
test_id = test['Id']
test = test.drop(['Id'], axis=1)
train_2 = train.drop(['SalePrice', 'Id'], axis=1)

# Concatenating train & test set

train_test = pd.concat([train_2, test], axis=0, sort=False)

# Looking at NaN % within the data

nan = pd.DataFrame(train_test.isna().sum(), columns=['NaN_sum'])
nan['feat'] = nan.index
nan['Perc(%)'] = (nan['NaN_sum']/1460)*100
nan = nan[nan['NaN_sum'] > 0]
nan = nan.sort_values(by=['NaN_sum'])
nan['Usability'] = np.where(nan['Perc(%)'] > 20, 'Discard', 'Keep')
nan

"""After reading data_description we can that some of NaNs are not missing information, so we should transform them to categorical."""

# Converting non-numeric predictors stored as numbers into string

train_test['MSSubClass'] = train_test['MSSubClass'].apply(str)
train_test['YrSold'] = train_test['YrSold'].apply(str)
train_test['MoSold'] = train_test['MoSold'].apply(str)

# Filling Categorical NaN

train_test['Functional'] = train_test['Functional'].fillna('Typ')
train_test['Electrical'] = train_test['Electrical'].fillna("SBrkr")
train_test['KitchenQual'] = train_test['KitchenQual'].fillna("TA")
train_test['Exterior1st'] = train_test['Exterior1st'].fillna(
    train_test['Exterior1st'].mode()[0])
train_test['Exterior2nd'] = train_test['Exterior2nd'].fillna(
    train_test['Exterior2nd'].mode()[0])
train_test['SaleType'] = train_test['SaleType'].fillna(
    train_test['SaleType'].mode()[0])
train_test["PoolQC"] = train_test["PoolQC"].fillna("None")
train_test["Alley"] = train_test["Alley"].fillna("None")
train_test['FireplaceQu'] = train_test['FireplaceQu'].fillna("None")
train_test['Fence'] = train_test['Fence'].fillna("None")
train_test['MiscFeature'] = train_test['MiscFeature'].fillna("None")

for col in ('GarageArea', 'GarageCars'):
    train_test[col] = train_test[col].fillna(0)

for col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:
    train_test[col] = train_test[col].fillna('None')

for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1',
            'BsmtFinType2'):
    train_test[col] = train_test[col].fillna('None')

# Checking the features with NaN remained out

for col in train_test:
    if train_test[col].isna().sum() > 0:
        print(train_test[col][0])


# Imputing with KnnRegressor (we can also use different Imputers)

def impute_knn(df):
    ttn = train_test.select_dtypes(include=[np.number])
    ttc = train_test.select_dtypes(exclude=[np.number])

    cols_nan = ttn.columns[ttn.isna().any()].tolist()  # columns w/ nan
    cols_no_nan = ttn.columns.difference(cols_nan).values  # columns w/n nan

    for col in cols_nan:
        imp_test = ttn[ttn[col].isna()]
        imp_train = ttn.dropna()
        model = KNeighborsRegressor(n_neighbors=5)
        knr = model.fit(imp_train[cols_no_nan], imp_train[col])
        ttn.loc[ttn[col].isna(), col] = knr.predict(imp_test[cols_no_nan])

    return pd.concat([ttn, ttc], axis=1)


train_test = impute_knn(train_test)


objects = []
for i in train_test.columns:
    if train_test[i].dtype == object:
        objects.append(i)
train_test.update(train_test[objects].fillna('None'))

# Checking NaN presence

for col in train_test:
    if train_test[col].isna().sum() > 0:
        print(train_test[col][0])


train_test["SqFtPerRoom"] = (train_test["GrLivArea"] /
                             (train_test["TotRmsAbvGrd"] +
                              train_test["FullBath"] + train_test["HalfBath"] +
                              train_test["KitchenAbvGr"]))

train_test['Total_Home_Quality'] = (train_test['OverallQual'] +
                                    train_test['OverallCond'])

train_test['Total_Bathrooms'] = (train_test['FullBath'] +
                                 (0.5 * train_test['HalfBath']) +
                                 train_test['BsmtFullBath'] +
                                 (0.5 * train_test['BsmtHalfBath']))

train_test["HighQualSF"] = train_test["1stFlrSF"] + train_test["2ndFlrSF"]

# Converting non-numeric predictors stored as numbers into string

train_test['MSSubClass'] = train_test['MSSubClass'].apply(str)
train_test['YrSold'] = train_test['YrSold'].apply(str)
train_test['MoSold'] = train_test['MoSold'].apply(str)

# Creating dummy variables from categorical features

train_test_dummy = pd.get_dummies(train_test)

# Fetch all numeric features

# train_test['Id'] = train_test['Id'].apply(str)
numeric_features = train_test_dummy.dtypes[train_test_dummy.dtypes !=
                                           object].index
skewed_features = train_test_dummy[numeric_features].apply(
    lambda x: skew(x)).sort_values(ascending=False)
high_skew = skewed_features[skewed_features > 0.5]
skew_index = high_skew.index

# Normalize skewed features using log_transformation

for i in skew_index:
    train_test_dummy[i] = np.log1p(train_test_dummy[i])

"""**Modeling**"""

train = train_test_dummy[0:1460]
test = train_test_dummy[1460:]
test['Id'] = test_id

target_log = np.log1p(target)

X_train, X_val, y_train, y_val = train_test_split(train, target_log,
                                                  test_size=0.1,
                                                  random_state=42)

# Cat Boost Regressor

cat = CatBoostRegressor()
cat_model = cat.fit(X_train, y_train, eval_set=(X_val, y_val), plot=True,
                    verbose=0)


def rmse(y, y_pred):
    return np.sqrt(mean_squared_error(y, y_pred))


cat_pred = cat_model.predict(X_val)
cat_score = rmse(y_val, cat_pred)
cat_score

"""Feature importance."""

feat_imp = cat_model.get_feature_importance(prettified=True)
feat_imp

"""**Hyperparameter Optimization**"""

# Preforming a Random Grid Search to find the best combination of parameters

grid = {'iterations': [1000, 3000, 6000],
        'learning_rate': [0.0005 + 0.0005*i for i in range(0, 50)],
        'depth': [4, 6, 10],
        'l2_leaf_reg': [1, 3, 5, 9]}

final_model = CatBoostRegressor()
randomized_search_result = final_model.randomized_search(grid, X=X_train,
                                                         y=y_train,
                                                         verbose=False,
                                                         plot=True)

"""Final model."""

print("\n========================================================")
print(" Results from Grid Search ")
print("========================================================")
print("\n The best params across ALL searched params:\n",
      randomized_search_result['params'])

# Final Cat-Boost Regressor

params = {'iterations': randomized_search_result['params']['iterations'],
          'learning_rate': randomized_search_result['params']['learning_rate'],
          'depth': randomized_search_result['params']['depth'],
          'l2_leaf_reg': randomized_search_result['params']['l2_leaf_reg'],
          'eval_metric': 'RMSE',
          'early_stopping_rounds': 200,
          'verbose': 200,
          'random_seed': 17}

cat_f = CatBoostRegressor(**params)
cat_model_f = cat_f.fit(X_train, y_train, eval_set=(X_val, y_val),
                        plot=True, verbose=False)

catf_pred = cat_model_f.predict(X_val)
catf_score = rmse(y_val, catf_pred)
catf_score

"""**Saving model and submission**"""

# Commented out IPython magic to ensure Python compatibility.
# %%flake8
# # Test CSV Submission
# cat_model_f.save_model('House_Price_model')
# test_pred = cat_f.predict(test)
# submission = pd.DataFrame(test_id, columns=['Id'])
# test_pred = np.expm1(test_pred)
# submission['SalePrice'] = test_pred
# submission.head()
# submission.to_csv("result.csv", index=False, header=True)
# print('Done!')
# -*- coding: utf-8 -*-
"""Titanic.ipynb"

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aaicWi27KIIZD1w9Zbitt-1bzGTrNxXp
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
pd.options.mode.chained_assignment = None 
# %matplotlib inline
import warnings
warnings.filterwarnings('ignore')
!pip install catboost

pip install flake8 pycodestyle_magic

# Commented out IPython magic to ensure Python compatibility.
# %load_ext pycodestyle_magic

"""**1.  Load the Data**"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
pd.options.mode.chained_assignment = None
# %matplotlib inline
warnings.filterwarnings('ignore')

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

from google.colab import drive
drive.mount('/content/drive/')

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

train_data = drive.CreateFile({'id': '1TNpBMpZVCbvF6hgP-Gvu5Iybu1hiKJkS'})
train_data.GetContentFile('train.csv')
test_data = drive.CreateFile({'id': '1d2_B-6SFGKtBwAeV3THr0459ClHfsK5C'})
test_data.GetContentFile('test.csv')

train_data = pd.read_csv("train.csv", index_col="PassengerId")
test_data = pd.read_csv("test.csv", index_col="PassengerId")

"""1.   Let's see some info about train_data
2.   Let's see some info about test_data
"""

print(train_data.info())
print(train_data.isna().sum())  # amount of missied values for each column

print(test_data.info())
print(test_data.isna().sum())

"""**2. Visualising the Data**


Let's see how different variables are associated with survival variable

**1)Pclass variable**
"""

print(train_data["Pclass"].unique())
train_data[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean(
                                  ).sort_values(by='Survived', ascending=False)

"""There are three classes (1, 2 and 3) representing first, second or third class tickets on the boat.

As we can see with the higher Pclass passengers having a higher survival rate. 

So Pclass variable can make an impact on the final result and it makes sense to include it in the model.

**2)Name variable**
"""

print(train_data["Name"])

print(train_data.Name[1].split())

"""As we can see by splitting name we got title, surname and name of the person.

So I can suppose that the whole families either survived or died together. Maybe i'm wrong, but never mind:)

So i decided to match up surnames to group families together.

Also i wanna get titles of names in terms of there are some common ones such as Mr and Miss and some rare/unique ones such as Rev (reverend). 

I can assume that the passengers with their own titles were sagnificantly important so they might have been more likely to survive.

In order to get family name (surname) we can split name of passenger by comma and get the first entry of array.
"""

print(train_data.Name[1].split(",")[0])

"""In order to get title we can split name of passenger by comma and get the second entry of array.After split it by dot and get the first entry of array."""

print(train_data.Name[1].split(",")[1].split(".")[0])

train_data = train_data.assign(fname=train_data.Name.str.split(",").str[0])
train_data["title"] = pd.Series([i.split(",")[1].split(".")[0].strip()
                                for i in train_data.Name],
                                index=train_data.index)
test_data = test_data.assign(fname=test_data.Name.str.split(",").str[0])
test_data["title"] = pd.Series([i.split(",")[1].split(".")[0].strip()
                               for i in test_data.Name],
                               index=test_data.index)
train_data.drop("Name", axis=1, inplace=True)
test_data.drop("Name", axis=1, inplace=True)

"""Let's see the amount of unique fnames and titles."""

print(test_data.fname.nunique())
print(test_data.title.nunique())

"""Visualization for the number of values for each title."""

ts = sns.countplot(x="title", data=train_data)
ts = plt.setp(ts.get_xticklabels(), rotation=90)
print(train_data["title"].unique())
print(test_data["title"].unique())
other_titles = [title
                for title in train_data["title"]
                if title not in ["Mr", "Miss", "Master",
                                 "Mme", "Mlle", "Mrs", "Ms"]]
# these titles are more important than other common ones
other_titles.append("Dona")
print(set(other_titles))  # the unique values of other titles

"""There are a lot of uniques so I want to group them.

I will use the pandas dataframe replace and map functions for this.
"""

common = {"Mr": 0, "Miss": 1, "Ms": 1, "Mme": 1, "Mlle": 1,
                              "Mrs": 1, "Master": 2, "Other": 3}
train_data["title"] = train_data['title'].replace(other_titles, 'Other')
train_data["title"] = train_data["title"].map(common)
test_data["title"] = test_data['title'].replace(other_titles, 'Other')
test_data["title"] = test_data["title"].map(common)

print(train_data.title)
print(test_data.title.isna().sum())
print(train_data.title.nunique())

"""fname and title variables are not numerical so it makes sense to apply OneHotEncoder. 

We can not use map in this case in terms of our model will interpret these 
columns as quantitative variables and as the result will be inccorrect output.

We just wanna show to model the belonging of passenger to the specific family and title.
"""

from sklearn.preprocessing import OneHotEncoder
oh = OneHotEncoder(handle_unknown="ignore", sparse=False)
train_data = train_data.join(pd.DataFrame(oh.fit_transform(
    train_data[["fname", "title"]]), index=train_data.index))
test_data = test_data.join(pd.DataFrame(oh.transform(
    test_data[["fname", "title"]]), index=test_data.index))
train_data.drop("fname", axis=1, inplace=True)
test_data.drop("fname", axis=1, inplace=True)

"""**3)Sex variable**"""

print(train_data["Sex"].unique())
train_data[['Sex', 'Survived']].groupby(['Sex'],
                                        as_index=False).mean().sort_values(
                                            by='Survived', ascending=False)

"""As we can see females have a much higher survival rate. 

It makes sense to include Sex in the model.

Across all males and females, females have a much higher survival rate. 

But what if wealthy males have a higher survival than poor females? It might make sense to segment this out explicity.

Let's create a new feature sex_class that represents all above.
"""

interactions = train_data.assign(sex_class=train_data['Sex'] + "_" +
                                 train_data['Pclass'].astype("str"))
interactions[['sex_class', 'Survived']].groupby(
             ['sex_class'], as_index=False).mean().sort_values(
                            by='Survived', ascending=False)

train_data = train_data.assign(sex_class=train_data['Sex'] + "_" +
                               train_data['Pclass'].astype("str"))
test_data = test_data.assign(sex_class=test_data['Sex'] + "_" +
                             test_data['Pclass'].astype("str"))
print(train_data)

"""Pclass variable is encoded numerically, but it is ordinal and it will be treated the same as something like Age by most of the models.

So let's encode it by using dummy variables.
"""

train_data = train_data.join(pd.get_dummies(train_data['Pclass'],
                                            prefix="Pclass"))
test_data = test_data.join(pd.get_dummies(test_data['Pclass'],
                                          prefix="Pclass"))
print(train_data)

"""Let's encode Sex variable as numeric by using the map method."""

train_data["Sex"] = train_data["Sex"].map({"female": 0, "male": 1})
test_data["Sex"] = test_data["Sex"].map({"female": 0, "male": 1})

"""The same stuff for sex_class."""

sex_class = {"female_1": 0, "female_2": 1, "female_3": 2,
             "male_1": 4, "male_2": 5, "male_3": 6}
train_data["sex_class"] = train_data["sex_class"].map(sex_class)
test_data["sex_class"] = test_data["sex_class"].map(sex_class)
print(train_data.sex_class)

"""**4)Age**

Let's look at the distribution of age and see if there is any association with survival.
"""

g = sns.FacetGrid(train_data, col='Survived')
g = g.map(sns.distplot, "Age")

print(train_data.Age.isna().sum())

"""There are some missing values that need to be dealt with. 

So we can replace the missing data with the average from similar passengers. 

For example, if we're missing the age of a 1st class passenger, who is female, who embarked from C. 

We could substitute in the age of other passengers who fit that description.
"""

def find_similar_passengers(id, dataset):
    subset = dataset[(dataset.title == dataset.title[id]) &
                     (dataset.Pclass == dataset.Pclass[id])]

    if subset["Age"].mean() == "NaN":
        subset = dataset[(dataset["sex_class"] ==
                          dataset.iloc[id]["sex_class"])]

    if subset["Age"].mean() == "NaN":
        subset = dataset[(dataset["sex"] == dataset.iloc[id]["sex"])]

    age = subset["Age"].mean()
    return age

no_ages = train_data[train_data["Age"].isna()].index
for pid in no_ages:
    train_data.Age[pid] = find_similar_passengers(pid, train_data)

no_ages_test = test_data[test_data["Age"].isna()].index
for pid2 in no_ages_test:
    test_data.Age[pid2] = find_similar_passengers(pid2, test_data)

"""The missing data is filled in.

I think that children have a much higher survival rate and the elderly have a much lower.

So we can reorganise Age column by segmenting it into groups of <5, 5-65 and >65.

"""

train_data["age_group"] = pd.cut(train_data["Age"], bins=[0, 5, 65, 100],
                                 labels=[0, 1, 2]).astype("int64")
test_data["age_group"] = pd.cut(test_data["Age"], bins=[0, 5, 65, 100],
                                labels=[0, 1, 2]).astype("int64")
print(train_data)

"""**5)SibSp and Parch variables**

SibSp: The number of siblings or spouses aboard the titanic.

Parch: The number of parents/children aboard the titanic.

Both of them have the straight relation to family size, so we can add them together.
"""

train_data[['SibSp', 'Survived']].groupby(['SibSp'],
                                          as_index=False).mean().sort_values(
                                              by='Survived', ascending=False)

train_data[['Parch', 'Survived']].groupby(['Parch'],
                                          as_index=False).mean().sort_values(
                                              by='Survived', ascending=False)

"""As we can see that smaller families tended to survive more than larger families.

"""

train_data["fsize"] = train_data["SibSp"] + train_data["Parch"] + 1
test_data["fsize"] = test_data["SibSp"] + test_data["Parch"] + 1

train_data[['fsize', 'Survived']].groupby(['fsize'],
                                          as_index=False).mean().sort_values(
                                              by='Survived', ascending=False)

"""Small families (4 or less) survived better than people who were alone or in bigger families.

**6)Ticket**
"""

print(train_data.Ticket.nunique())
print(train_data.Ticket.isna().sum())
print(train_data.Ticket.tail())

"""As we can see tickets are numbers with some prefix letters.

Let's separate them.
"""

train_data["ticket_prefix"] = pd.Series([len(i.split()) > 1 for i in
                                         train_data.Ticket],
                                        index=train_data.index)

train_data[['ticket_prefix', 'Survived']].groupby(
    ['ticket_prefix'], as_index=False).mean().sort_values(
        by='Survived', ascending=False)

"""The survival variable doesn't depand on weather ticket has prefix or not. 

I can conclude that ticket variable makes no impact on the result.

So let's get rid of it.
"""

train_data.drop("ticket_prefix", axis=1, inplace=True)
train_data.drop("Ticket", axis=1, inplace=True)
test_data.drop("Ticket", axis=1, inplace=True)

"""**7)Fare**"""

g = sns.FacetGrid(train_data, col='Survived')
g = g.map(sns.distplot, "Fare")

"""You can see that survivors had more expensive fares and a wider spread of fare prices. 

There is at least one outlier with a fare of >500 so dropping it.

The data is pretty skewed. Take a log transformation to reduce the skew and to decrease the massive range in fares.
"""

import numpy as np
train_data["Fare"] = train_data["Fare"].map(
                     lambda i: np.log(i) if i > 0 else 0)
test_data["Fare"] = test_data["Fare"].map(lambda i: np.log(i) if i > 0 else 0)

g = sns.FacetGrid(train_data, col='Survived')
g = g.map(sns.distplot, "Fare")

"""**8)Cabin**

In terms of a big amout of missing values i decided to drop this column.

Also i think it makes no impact on the result.
"""

train_data.drop("Cabin", axis=1, inplace=True)
test_data.drop("Cabin", axis=1, inplace=True)

"""**9)Embarked**
 
 Let's fill in missing values as S.

 And encode it by using dummy variable.
"""

train_data["Embarked"] = train_data["Embarked"].fillna("S")
train_data[['Embarked', 'Survived']].groupby(
    ['Embarked'], as_index=False).mean().sort_values(
        by='Survived', ascending=False)

print(train_data.Embarked.isna().sum())

train_data = train_data.join(pd.get_dummies(train_data['Embarked'],
                                            prefix="Embarked_"))
test_data = test_data.join(pd.get_dummies(test_data['Embarked'],
                                          prefix="Embarked_"))

train_data.drop("Embarked", axis=1, inplace=True)
test_data.drop("Embarked", axis=1, inplace=True)

"""**3)Data Normalization/Standartization.**

Firstly, let's split our dataframes up into the independant variables (matrix X) 

and the dependant variable (the vector y).

In terms of our independant variables have different scale we are supposed 

to normalize it by using StandartScaler(Z-normalization).
"""

from sklearn.preprocessing import StandardScaler
ss = StandardScaler()

train_y = train_data["Survived"]
train_data.drop("Survived", axis=1, inplace=True)

scoring_method = "f1"

train_scaled = ss.fit_transform(train_data)
test_scaled = ss.transform(test_data)

print(train_data.isna().sum())
print(test_data.isna().sum())

"""I can conclude that our data is prepared for using.

**4)Modeling**

For each model i used GridSeachCV that allows me to create 

grid of possible values for the parameters and it will test 

all possible combinations, storing the best result.

So i won't search the best parameters manually, but it's brude force algorithm and it will take a while to finish.

*1)LogisticRegression*
"""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
import numpy as np
model = LogisticRegression(random_state=10, max_iter=1000)
logit_params = {
    "C": [1, 3, 10, 20, 30, 40],
    "solver": ["lbfgs", "liblinear"]
}
logit_gs = GridSearchCV(model, logit_params, scoring="f1", cv=5, n_jobs=4)

logit_gs.fit(train_data, train_y)

print(logit_gs.best_params_)
print(logit_gs.best_score_)

"""*2)RandomForest*"""

from sklearn.ensemble import RandomForestClassifier
rf_model = RandomForestClassifier()

rf_params = {
    'bootstrap': [True, False],
    'max_depth': [10, None],
    'max_features': ['auto', 'sqrt'],
    'min_samples_leaf': [1, 2, 4],
    'min_samples_split': [2, 5, 10],
    'n_estimators': [5, 10, 15, 20, 25, 30]}

rf_gs = GridSearchCV(rf_model, rf_params,
                     scoring=scoring_method, cv=8, n_jobs=4)

rf_gs.fit(train_data, train_y)

print(rf_gs.best_params_)
print(rf_gs.best_score_)

"""*3)Support Vector Machine*"""

from sklearn.svm import SVC
svc_model = SVC()

test_parameters = {
    "C": [1, 3, 10, 30, 100],
    "kernel": ["linear", "poly", "rbf", "sigmoid"],
}
svc_gs = GridSearchCV(svc_model, test_parameters, scoring="f1", cv=5, n_jobs=4)

svc_gs.fit(train_scaled, train_y)

print(svc_gs.best_params_)
print(svc_gs.best_score_)

"""*4)Light Gradient Boosting*"""

from lightgbm import LGBMClassifier
lgb_model = LGBMClassifier()
test_parameters = {
    "n_estimators": [int(x) for x in np.linspace(5, 30, 6)],
    "reg_alpha": [0, 0.75, 1, 1.25],
    "learning_rate": [0.5, 0.4, 0.35, 0.3, 0.25, 0.2],
    "subsample": [0.5, 0.75, 1]
}
lgb_gs = GridSearchCV(lgb_model, test_parameters,
                      scoring=scoring_method, cv=8, n_jobs=4)

lgb_gs.fit(train_data, train_y)

print(lgb_gs.best_params_)
print(lgb_gs.best_score_)

"""*5)XGBoost*"""

from xgboost import XGBClassifier
xgb_model = XGBClassifier()

parameters = {'nthread': [4],
              'objective': ['binary:logistic'],
              'learning_rate': [0.05],
              'max_depth': [6],
              'min_child_weight': [11],
              'silent': [1],
              'subsample': [0.8],
              'colsample_bytree': [0.7],
              'n_estimators': [5],
              'missing': [-999],
              'seed': [1337]}


xgb_gs = GridSearchCV(xgb_model, parameters,
                      scoring=scoring_method, cv=8, n_jobs=5)

xgb_gs.fit(train_data, train_y)

print(xgb_gs.best_params_)
print(xgb_gs.best_score_)

"""*6)CatBoost*"""

from catboost import CatBoostClassifier

catBoost = CatBoostClassifier()
test_parameters = {'iterations': [500],
                   'depth': [4, 5, 6],
                   'loss_function': ['Logloss', 'CrossEntropy'],
                   'l2_leaf_reg': np.logspace(-20, -19, 3),
                   'leaf_estimation_iterations': [10],
                   'logging_level': ['Silent'],
                   'random_seed': [42]
                   }
catboost_gs = GridSearchCV(catBoost, test_parameters,
                           scoring=scoring_method, cv=5, n_jobs=4)

catboost_gs.fit(train_data, train_y)

print(catboost_gs.best_params_)
print(catboost_gs.best_score_)

"""**5)Comparing models.**

These different models are probably placing different levels of importance on different features/variables. 

I think the key to a good ensembler/voter is to have models that have different predictions.

**6)Ensembling/Voting**

Let's use a voting classifier to use above models to make an overall prediction. 

I don't include XGBoost and CatBoost by the reason of overfitting.

When i include them i get 99 percent accuracy, but kaggle gives me worse results for this ensemble.

But without it ii get better results on keggle.

Am i correct of naming it as overfitting?

Also i faced with the problem that VotingClassifier no longer willing 

to apply all of the models during fit function working. 

The result on kaggle would be better if it worked properly.
"""

from sklearn.ensemble import VotingClassifier

ensemble_model = VotingClassifier(estimators=[
    ("logit", logit_gs.best_estimator_),
    ("rf", rf_gs.best_estimator_),
    ("svc", svc_gs.best_estimator_),
    ("lgb", lgb_gs.best_estimator_),
    # ("xgb", xgb_gs.best_estimator_),
    # ("catboost", catboost_gs.best_estimator_),
], voting="hard")

print(ensemble_model.estimators)

ensemble_model.fit(train_data, train_y)

ensemble_model.score(train_data, train_y)

preds = ensemble_model.predict(test_data)

output = pd.DataFrame({'PassengerId': test_data.index,
                       'Survived': preds})

output.to_csv('submission.csv', index=False)
# -*- coding: utf-8 -*-
"""face_mask_detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17XGIAMpjN_AKLn6-l48mO7B_wxt12vFv

**This Notebook contains two tasks**

*(Custom models with accuracy more than 85% and transfer learning)*

**Importing libraries**
"""

import os
import cv2
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers
from google.colab import files
from zipfile import ZipFile
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.applications import VGG16
from tensorflow.keras.applications import VGG19
from tensorflow.keras.applications import InceptionV3
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.applications import InceptionResNetV2

"""**Loading the data**"""

!pip install kaggle

files.upload()

! mkdir ~/.kaggle
 ! cp kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d omkargurav/face-mask-dataset

file_name = 'face-mask-dataset.zip'

with ZipFile(file_name, 'r') as zip:
  zip.extractall()
  print("Done")

batch_size = 40
img_height = 200
img_width = 200

train_data = tf.keras.preprocessing.image_dataset_from_directory(
    'data',
    validation_split=0.2,
    subset="training",
    seed=42,
    image_size=(img_height, img_width),
    batch_size=batch_size
)

test_data = tf.keras.preprocessing.image_dataset_from_directory(
    'data',
    validation_split=0.2,
    subset="validation",
    seed=42,
    image_size=(img_height, img_width),
    batch_size=batch_size
)

class_names = train_data.class_names

plt.figure(figsize=(10, 10))
for images, labels in train_data.take(1):
  for i in range(12):
    ax = plt.subplot(3, 4, i + 1)
    plt.imshow(images[i].numpy().astype("uint8"))
    plt.title(class_names[labels[i]])
    plt.grid(True)

"""**Configuring datasets for performance**"""

AUTOTUNE = tf.data.experimental.AUTOTUNE
train_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)
test_data= test_data.cache().prefetch(buffer_size=AUTOTUNE)

"""**Modeling**

Using flatten layer will increase the size of the model,

hence use Global avg Pooling instead.

Is it necessary to include input_shape in the first layer?

Does it affect the performance?
"""

model = tf.keras.models.Sequential([
  layers.experimental.preprocessing.Rescaling(1./255),
  layers.Conv2D(32, 3, activation='relu'),#,input_shape=(200,200,3)),
  layers.MaxPooling2D(),
  layers.Conv2D(64, 3, activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(128, 3, activation='relu'),
  layers.MaxPooling2D(),
  layers.GlobalAveragePooling2D(),
  layers.Dense(256, activation='relu'),
  layers.Dense(2, activation= 'softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

"""**Training my CNN**"""

retVal = model.fit(train_data, validation_data=test_data, epochs=15)

"""**Let's visualize the result of training.**"""

plt.plot(retVal.history['loss'], label = 'training loss')
plt.plot(retVal.history['accuracy'], label = 'training accuracy')
plt.legend()

"""Is it bad when the learning curve looks almost like a line?

Let's match the predicted labels with  real images.
"""

plt.figure(figsize=(20, 20))
for images, labels in test_data.take(1):
    predictions = model.predict(images)
    predlabel = []
    
    for mem in predictions:
        predlabel.append(class_names[np.argmax(mem)])
    
    for i in range(40):
        ax = plt.subplot(10, 4, i + 1)
        plt.imshow(images[i].numpy().astype("uint8"))
        plt.title('Predicted label:'+ predlabel[i])
        plt.axis('off')
        plt.grid(True)

"""Checking for scores."""

scores = model.evaluate(test_data, verbose=0)
print(scores[1]*100)

"""**Experimenting with different structures of model**

In this one flatten was used.
"""

model_1 = tf.keras.models.Sequential([
  layers.experimental.preprocessing.Rescaling(1./255),
  layers.Conv2D(32, 3, padding='same', activation='relu'),
  layers.Conv2D(32, 3, activation='relu'),
  layers.MaxPooling2D(),
  layers.Dropout(0.25),
  layers.Conv2D(64, 3, padding='same', activation='relu'),
  layers.Conv2D(64, 3, activation='relu'),
  layers.MaxPooling2D(),
  layers.Dropout(0.25),     
  layers.Flatten(),
  layers.Dense(512, activation='relu'),
  layers.Dropout(0.5),
  layers.Dense(2, activation='softmax')                                
])

model_1.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

"""**Training the model.**"""

retVal_1 = model_1.fit(train_data, validation_data=test_data, epochs=15)

"""It took me a little bit more time for training in case of using flatten which increased the size of the model.

Am i correct?

Visualizing the trainging process
"""

plt.plot(retVal_1.history['loss'], label = 'training loss')
plt.plot(retVal_1.history['accuracy'], label = 'training accuracy')
plt.legend()

"""I think this curves look better than previous ones, aren't them?

Let's match the predicted labels with real images.
"""

plt.figure(figsize=(20, 20))
for images, labels in test_data.take(1):
    predictions = model_1.predict(images)
    predlabel = []
    
    for mem in predictions:
        predlabel.append(class_names[np.argmax(mem)])
    
    for i in range(40):
        ax = plt.subplot(10, 4, i + 1)
        plt.imshow(images[i].numpy().astype("uint8"))
        plt.title('Predicted label:'+ predlabel[i])
        plt.axis('off')
        plt.grid(True)

scores = model_1.evaluate(test_data, verbose=0)
print(scores[1]*100)

"""The last custom model with 3 layers of convolutions and subsamplings"""

model_2 = tf.keras.models.Sequential([
  layers.experimental.preprocessing.Rescaling(1./255),

  layers.Conv2D(32, 3, activation='relu'),
  layers.MaxPooling2D(),

  layers.Conv2D(32, 3, activation='relu'),
  layers.MaxPooling2D(),

  layers.Conv2D(64, 3, activation='relu'),
  layers.MaxPooling2D(),
  
  layers.Flatten(),
  layers.Dense(64, activation='relu'),
  layers.Dropout(0.5),
  layers.Dense(1, activation='sigmoid')                                
])

model_2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

retVal_2 = model_2.fit(train_data, validation_data=test_data, epochs=15)

plt.plot(retVal_2.history['loss'], label = 'training loss')
plt.plot(retVal_2.history['accuracy'], label = 'training accuracy')
plt.legend()

plt.figure(figsize=(20, 20))
for images, labels in test_data.take(1):
    predictions = model_2.predict(images)
    predlabel = []
    
    for mem in predictions:
        predlabel.append(class_names[np.argmax(mem)])
    
    for i in range(40):
        ax = plt.subplot(10, 4, i + 1)
        plt.imshow(images[i].numpy().astype("uint8"))
        plt.title('Predicted label:'+ predlabel[i])
        plt.axis('off')
        plt.grid(True)

scores = model_2.evaluate(test_data, verbose=0)
print(scores[1]*100)

"""**Transfer learning**

Let's use VGG16,VGG19, Incsption v3, ResNet50, Xception CNNs.

**VGG16**

include_top=False means that won't include the part of the VGG16 that is dedicated to classification.

(Only Convolutional part will be included)

weights='imagenet' means that we want to use learnt weights from imagenet dataset.
"""

VGG16_model = VGG16(weights='imagenet', include_top=False, input_shape=(200, 200, 3))

"""trainable=False means that we don't want to teach convolutional part of the model."""

VGG16_model.trainable = False

"""Let's see all layers and amount of parameters of the vgg16."""

VGG16_model.summary()

"""Let's create model based on vgg16"""

model_3 = tf.keras.models.Sequential([
  #layers.experimental.preprocessing.Rescaling(1./255),
  VGG16_model,
  layers.Flatten(),
  layers.Dense(256, activation='relu'),
  layers.Dropout(0.5),
  layers.Dense(1, activation='sigmoid')                                
])

model_3.summary()

"""In terms of vgg16 had been already trained we need to set a low learing rate for convergence.

"""

model_3.compile(optimizer=Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])

"""Training model. 5 epochs will be enought due to part of the model had been already trained

"""

retVal_3 = model_3.fit(train_data, validation_data=test_data, epochs=5)

plt.plot(retVal_3.history['loss'], label = 'training loss')
plt.plot(retVal_3.history['accuracy'], label = 'training accuracy')
plt.legend()

plt.figure(figsize=(20, 20))
for images, labels in test_data.take(1):
    predictions = model_3.predict(images)
    predlabel = []
    
    for mem in predictions:
        predlabel.append(class_names[np.argmax(mem)])
    
    for i in range(40):
        ax = plt.subplot(10, 4, i + 1)
        plt.imshow(images[i].numpy().astype("uint8"))
        plt.title('Predicted label:'+ predlabel[i])
        plt.axis('off')
        plt.grid(True)

scores = model_3.evaluate(test_data, verbose=0)
print(scores[1]*100)

"""**VGG19**"""

VGG19_model = VGG19(weights='imagenet', include_top=False, input_shape=(200, 200, 3))

"""trainable=False means that we don't want to teach convolutional part of the model."""

VGG19_model.trainable = False

"""Let's see all layers and amount of parameters of the vgg16."""

VGG19_model.summary()

"""Let's create model based on vgg19"""

model_4 = tf.keras.models.Sequential([
  VGG19_model,
  layers.Flatten(),
  layers.Dense(256, activation='relu'),
  layers.Dropout(0.5),
  layers.Dense(1, activation='sigmoid')                                
])

model_4.summary()

"""In terms of vgg19 had been already trained we need to set a low learing rate for convergence.

"""

model_4.compile(optimizer=Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])

"""Training model. 5 epochs will be enought due to part of the model had been already trained

"""

retVal_4 = model_4.fit(train_data, validation_data=test_data, epochs=5)

plt.plot(retVal_4.history['loss'], label = 'training loss')
plt.plot(retVal_4.history['accuracy'], label = 'training accuracy')
plt.legend()

plt.figure(figsize=(20, 20))
for images, labels in test_data.take(1):
    predictions = model_4.predict(images)
    predlabel = []
    
    for mem in predictions:
        predlabel.append(class_names[np.argmax(mem)])
    
    for i in range(40):
        ax = plt.subplot(10, 4, i + 1)
        plt.imshow(images[i].numpy().astype("uint8"))
        plt.title('Predicted label:'+ predlabel[i])
        plt.axis('off')
        plt.grid(True)

scores = model_4.evaluate(test_data, verbose=0)
print(scores[1]*100)

"""**InceptionV3**"""

incV3_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(200, 200, 3))

"""trainable=False means that we don't want to teach convolutional part of the model."""

incV3_model.trainable = False

"""Let's see all layers and amount of parameters of the InceptionV3."""

incV3_model.summary()

"""Let's create model based on InceptionV3"""

model_5 = tf.keras.models.Sequential([
  incV3_model,
  layers.Flatten(),
  layers.Dense(256, activation='relu'),
  layers.Dropout(0.5),
  layers.Dense(1, activation='sigmoid')                                
])

model_5.summary()

"""In terms of InceptionV3 had been already trained we need to set a low learing rate for convergence.

"""

model_5.compile(optimizer=Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])

"""Training model. 5 epochs will be enought due to part of the model had been already trained

"""

retVal_5 = model_5.fit(train_data, validation_data=test_data, epochs=5)

plt.plot(retVal_5.history['loss'], label = 'training loss')
plt.plot(retVal_5.history['accuracy'], label = 'training accuracy')
plt.legend()

plt.figure(figsize=(20, 20))
for images, labels in test_data.take(1):
    predictions = model_5.predict(images)
    predlabel = []
    
    for mem in predictions:
        predlabel.append(class_names[np.argmax(mem)])
    
    for i in range(40):
        ax = plt.subplot(10, 4, i + 1)
        plt.imshow(images[i].numpy().astype("uint8"))
        plt.title('Predicted label:'+ predlabel[i])
        plt.axis('off')
        plt.grid(True)

scores = model_5.evaluate(test_data, verbose=0)
print(scores[1]*100)

"""**ResNet50**"""

ResNet50_model = VGG16(weights='imagenet', include_top=False, input_shape=(200, 200, 3))

"""trainable=False means that we don't want to teach convolutional part of the model."""

ResNet50_model.trainable = False

"""Let's see all layers and amount of parameters of the vgg16."""

ResNet50_model.summary()

"""Let's create model based on ResNet50"""

model_6 = tf.keras.models.Sequential([
  ResNet50_model,
  layers.Flatten(),
  layers.Dense(256, activation='relu'),
  layers.Dropout(0.5),
  layers.Dense(1, activation='sigmoid')                                
])

model_6.summary()

"""In terms of ResNet50 had been already trained we need to set a low learing rate for convergence.

"""

model_6.compile(optimizer=Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])

"""Training model. 5 epochs will be enought due to part of the model had been already trained

"""

retVal_6 = model_6.fit(train_data, validation_data=test_data, epochs=5)

plt.plot(retVal_6.history['loss'], label = 'training loss')
plt.plot(retVal_6.history['accuracy'], label = 'training accuracy')
plt.legend()

plt.figure(figsize=(20, 20))
for images, labels in test_data.take(1):
    predictions = model_6.predict(images)
    predlabel = []
    
    for mem in predictions:
        predlabel.append(class_names[np.argmax(mem)])
    
    for i in range(40):
        ax = plt.subplot(10, 4, i + 1)
        plt.imshow(images[i].numpy().astype("uint8"))
        plt.title('Predicted label:'+ predlabel[i])
        plt.axis('off')
        plt.grid(True)

scores = model_6.evaluate(test_data, verbose=0)
print(scores[1]*100)

"""**InceptionV4/InceptionResNet**"""

InceptionV4_model = InceptionResNetV2(weights='imagenet', include_top=False, input_shape=(200, 200, 3))

InceptionV4_model.trainable = False

InceptionV4_model.summary()

model_7 = tf.keras.models.Sequential([
  InceptionV4_model,
  layers.Flatten(),
  layers.Dense(256, activation='relu'),
  layers.Dropout(0.5),
  layers.Dense(1, activation='sigmoid')                                
])

model_7.summary()

model_7.compile(optimizer=Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])

retVal_7 = model_7.fit(train_data, validation_data=test_data, epochs=5)

plt.plot(retVal_7.history['loss'], label = 'training loss')
plt.plot(retVal_7.history['accuracy'], label = 'training accuracy')
plt.legend()

plt.figure(figsize=(20, 20))
for images, labels in test_data.take(1):
    predictions = model_7.predict(images)
    predlabel = []
    
    for mem in predictions:
        predlabel.append(class_names[np.argmax(mem)])
    
    for i in range(40):
        ax = plt.subplot(10, 4, i + 1)
        plt.imshow(images[i].numpy().astype("uint8"))
        plt.title('Predicted label:'+ predlabel[i])
        plt.axis('off')
        plt.grid(True)

scores = model_7.evaluate(test_data, verbose=0)
print(scores[1]*100)

"""Saving **models**"""

model.save('maskDetector.h5')
model_1.save('maskDetector_1.h5')
model_2.save('maskDetector_2.h5')
model_3.save('maskDetector_3.h5')
model_4.save('maskDetector_4.h5')
model_5.save('maskDetector_5.h5')
model_6.save('maskDetector_6.h5')
model_7.save('maskDetector_7.h5')